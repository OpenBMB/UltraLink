<div align="center">

<img src="title.png" alt="UltraLink" width="200">

**multi-lingual, knowledge-grounded, multi-round dialogue dataset and model**

<p align="center">
 <a href="#Introduction"> Introduction </a> â€¢
 <a href="#Construction-of-UltraLink">Construction Process</a> â€¢
 <a href="https://arxiv.org/abs/2402.04588">Paper</a> â€¢
 <a href="https://huggingface.co/datasets/R0k1e/UltraLink"> UltraLink</a> â€¢
 <a href="https://huggingface.co/R0k1e/UltraLink-LM"> UltraLink-LM</a>
</p>
</div>

## News
- â—ï¸â—ï¸ Febrary 6, 2024: Releasing a multi-lingual, knowledge-grounded data augmented, multi-round dialogue dataset UltraLink and the model weight of UltraLink-LM.

## Introduction
### UltraLink
UltraLink is a multi-lingual, knowledge-grounded data augmented, multi-round dialogue dataset. It contains language-specific chat data, language-agnostic chat data, code data and math data in 5 languages: English, Chinese, Spanish, Russian, and French. It can be downloaded in this huggingface [link](https://huggingface.co/datasets/R0k1e/UltraLink).
Different from previous works that simply translate English instructions, we consider both the language-specific and language-agnostic abilities of LLMs. Firstly, we introduce a knowledge-grounded data augmentation approach to elicit more culture-specific knowledge of LLMs, improving their ability to serve users from different countries. Moreover, we find modern LLMs possess strong cross-lingual transfer capabilities, thus repeatedly learning identical content in various languages is not necessary. Consequently, we can substantially prune the language-agnostic SFT data without any performance degradation, making multilingual SFT more efficient. 

### UltraLink-LM
> The UltraLink-LM is a massively multilingual generative language model that follows instructions in 5 languages, English, French, Russian, Spanish, and Chinese. The model is capable of generating text in 5 languages with high quality and diversity.
> UltraLink-LM outperforms [PolyLM-Chat-13b](https://huggingface.co/DAMO-NLP-MT/polylm-chat-13b), [Guanaco](JosephusCheung/Guanaco),  and [Bloomz-7b1-mt](https://huggingface.co/bigscience/bloomz-7b1-mt) in code, math and chat abilities in four languages, and has a high-quality and diverse text generation performance in all languages.
> The UltraLink-LM is trained using [UltraLink](https://huggingface.co/datasets/R0k1e/UltraLink), [UltraChat](https://huggingface.co/datasets/stingning/ultrachat), [Magicoder-Evol](https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K), [Magicoder-OSS](https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K), [MetaMathQA](https://huggingface.co/datasets/meta-math/MetaMathQA), and [ShareGPT](https://huggingface.co/datasets/openchat/openchat_sharegpt4_dataset/).
> We release the checkpoints under a MIT license to further our mission of multilingual technologies empowering a multilingual world. It can be downloaded in this huggingface [link](https://huggingface.co/R0k1e/UltraLink-LM).

- **Developed by:** [OpenBMB]((https://www.openbmb.cn/home))
- **Model type:** a Transformer style autoregressive massively multilingual language model.
- **Paper**: [UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset](https://arxiv.org/abs/2402.04588)
- **Languages**: Refer to the list of languages in the `language` section of this model card.
- **License**: MIT
- **Model**: [UltraLink-LM](https://huggingface.co/R0k1e/UltraLink-LM)
- **Model Size**: 13 billion parameters
- **Datasets**: [UltraLink](https://huggingface.co/datasets/R0k1e/UltraLink), [UltraChat](https://huggingface.co/datasets/stingning/ultrachat)(random select 10k samples), [Magicoder-Evol](https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K), [Magicoder-OSS](https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K), [MetaMathQA](https://huggingface.co/datasets/meta-math/MetaMathQA), and [ShareGPT](https://huggingface.co/datasets/openchat/openchat_sharegpt4_dataset/)(the English part of the dataset whose sample length is greater than 4k).

<details><summary> <b> Performance </b> </summary>
<p>

We report three evaluations in this section: multilingual HumanEval, MGSM, and OMGEval.
Evaluations of modern LLMs may be biased and affected by many factors, we are also actively working on more comprehensive evaluation methods.

### Multilingual HumanEval 

[HumanEval](https://github.com/openai/human-eval) is a well-known benchmark for evaluating the code ability of LLMs. It execute the code snippets generated by the model and evaluate their correctness.  Since there are no existing multilingual test set for code generation, we use GPT-3.5 with carefully-designed prompts to translation HumanEval into other languages.

|Model|En|Zh|Es|Ru|Fr|Avg|
|-----|---|---|---|---|---|---|
|Bloomz-7b1-mt | 8.5 | 7.3 | 6.1 | 8.5 | 6.1 | 7.3 |
|Phoenix-inst-chat-7b | 11.0 | 10.4 | 8.5 | 1.2 | 13.4 | 12.2 |
|PolyLM-Multialpaca-13b | 8.5 | 7.3 | 6.1 | 6.1 | 6.1 | 6.8 |
|PolyLM-Chat-13b | 10.4 | 7.9 | 6.1 | 7.3 | 8.5 | 8.1 |
|Chimera-inst-chat-13b| 14.6 | 13.4 | 14.6 | 12.8 | 14.0 | 13.9 |
|Okapi-7b | 12.2 | 11.0 | 8.5 | 8.5 | 8.5 | 9.8 |
|Guanaco-7b | 9.2 | 6.7 | 11.0 | 9.8 | 12.8 | 9.9 |
|Guanaco-13b| 18.3 | 15.9 | 9.8 | 8.5 | 14.6 | 12.2 |
|UltraLink-LM  | __60.4__ | __43.9__ | __40.9__ | __49.4__ | __39.6__ | __46.8__|


### MGSM

We employ [MGSM](https://github.com/google-research/url-nlp/tree/main/mgsm) to evaluate the math reasoning abilities, which is a multilingual benchmark. It compares the result with correct answers and evaluates the model's ability to perform mathematical reasoning.
|Model|En|Zh|Es|Ru|Fr|Avg|
|-----|---|---|---|---|---|---|
|Bloomz-7b1-mt | 2.8 | 1.6 | 2.0 | 0.4 | 2.8 | 1.7 |
|Phoenix-inst-chat-7b | 3.2 | 3.2 | 2.8 | 3.2 | 3.2 | 3.1 |
|PolyLM-Multialpaca-13b | 1.2 | 2.8 | 1.6 | 2.8 | 2.4 | 2.4 |
|PolyLM-Chat-13b | 10.8 | 6.4 | 4.8 | 4.4 | 5.6 | 5.3 |
|Chimera-inst-chat-13b  | 14.0 | 11.6 | 10.0 | 12.0 | 12.8 | 11.6 |
|Okapi-7b | 4.0 | 2.4 | 3.6 | 4.4 | 4.8 | 3.8 |
|Guanaco-7b | 4.0 | 1.6 | 3.2 | 2.8 | 4.4 | 3.0 |
|Guanaco-13b | 13.6 | 10.8 | 11.2 | 6.4 | 5.2 | 8.4 |
|UltraLink-LM| __70.4__ | __56.0__ | __70.4__ | __64.8__ | __63.6__ | __63.7__ |

### OMGEval
We use the [OMGEval](https://github.com/blcuicall/OMGEval) to evaluate the chat ability, which is a multilingual version of the widely-used English benchmark AlpacaEval.

|Model|En|Zh|Es|Ru|Fr|Avg|
|-----|---|---|---|---|---|---|
|Bloomz-7b1-mt | 0.0 | 0.9 | 0.1 | 0.5 | 0.3 | 0.4 |
|Phoenix-inst-chat-7b  | 6.9 | 13.3 | 7.4 | 2.9 | 8.1 | 7.7 |
|PolyLM-Multialpaca-13b  | 3.4 | 5.0 | 2.1 | 5.1 | 2.2 | 3.6 |
|PolyLM-Chat-13b | 7.7 | 14.0 | 6.1 | 5.5 | 4.8 | 7.6 |
|Chimera-inst-chat-13b | 15.5 | 9.7 | 11.8 | 13.7 | 13.8 | 12.9 |
|Okapi-7b | 8.8 | 6.2 | 5.0 | 12.1 | 8.7 | 8.2 |
|Guanaco-7b  | 4.6 | 3.8 | 0.4 | 1.8 | 1.2 | 2.4 |
|Guanaco-13b  |  __29.0__ | 8.6 | 16.9 | 15.4 | 17.3 | 17.5 |
|UltraLink-LM |  28.8 |  __21.9__ |  __23.5__ | __37.6__ | __29.0__ |  __28.2__  |

</p>
</details>

## Data

The dataset is intended solely for research and educational purposes and should not be construed as reflecting the opinions or views of the creators, owners, or contributors of this dataset. And it is distributed under the MIT license.


### Data Release
The data consists of four parts, code data, math data, language-specific chat data, and language-agnostic chat data. Each type of data has been marked using the title of each file. For example, the en_code.jsonl specifies the English code data.

- ğŸ¤— [Huggingface Datasets Host](https://huggingface.co/datasets/R0k1e/UltraLink)


### Data Format
Each line in the downloaded data file is a json dict containing the data id and dialogue data in a list format. The id is begin wizh the file name and connected with the sequence number. Below is an example line.

```
{"id": "zh_chat_specific_191", 
"data": [
      "è¯·æ ¹æ®ä»¥ä¸‹ç»™å®šçš„æ–‡æ¡£å›ç­”æˆ‘çš„é—®é¢˜ã€‚<document>æå¨…\n\næå¨…ï¼Œå®‰å¾½èšŒåŸ äººï¼Œä¸­å›½é€€å½¹å¥³å­ä½“æ“è¿åŠ¨å‘˜ï¼Œå¼ºé¡¹ä¸ºé«˜ä½æ å’Œå¹³è¡¡æœ¨ã€‚åœ¨ä¸­å›½ä½“æ“é˜Ÿæ˜µç§°â€œå°èƒ–â€ã€‚\nç”Ÿå¹³.\nè¿åŠ¨ç”Ÿæ¶¯.\næå¨…1993å¹´å¼€å§‹ç»ƒä¹ ä½“æ“ï¼Œ1999å¹´è¿›å…¥å®‰å¾½é˜Ÿï¼Œ2002å¹´å…¥é€‰å›½å®¶é˜Ÿï¼Œæ•™ç»ƒä¸ºåˆ˜æ¡‚æˆå’Œä½•èŠ±ã€‚2002å¹´æå¨…åœ¨å…¨å›½ä½“æ“å† å†›èµ›è·å¾—å¹³è¡¡æœ¨å† å†›ï¼Œå¼€å§‹æ–©éœ²å¤´è§’ã€‚2003å¹´æå¨…å‡ºå¾ç¬¬37å±Šç¾å›½å®‰é‚£ç½•ä¸–ç•Œä½“æ“é”¦æ ‡èµ›ï¼Œè·å¾—å›¢ä½“ç¬¬å››å’Œå¹³è¡¡æœ¨å•é¡¹ç¬¬å››ã€‚\n2004å¹´å¯¹äºæå¨…æ¥è¯´æ˜¯ä¸å¯»å¸¸çš„ä¸€å¹´ã€‚åœ¨è·å¾—å…¨å›½å¹³è¡¡æœ¨å† å†›å’Œå…¨èƒ½äºšå†›åï¼Œå¥¹å‚åŠ äº†é›…å…¸å¥¥è¿ä¼šï¼Œå‘æŒ¥å¤±å¸¸ã€‚éšååœ¨ä¸–ç•Œæ¯è‹±å›½æ ¼æ‹‰æ–¯å“¥å’Œæ¯”åˆ©æ—¶æ ¹ç‰¹ç³»åˆ—èµ›ä¸Šï¼Œå¥¹éƒ½è·å¾—äº†å¹³è¡¡æœ¨å† å†›ã€‚2004å¹´12æœˆï¼Œæå¨…åœ¨è‹±å›½ä¼¯æ˜ç¿°å¸‚ä¸¾è¡Œçš„ç¬¬12å±Šä¸–ç•Œæ¯æ€»å†³èµ›ä¸Šï¼Œæå¨…æ˜¯å‘æŒ¥æœ€å¥½çš„ä¸­å›½å¥³é€‰æ‰‹ï¼Œè·å¾—äº†å¹³è¡¡æœ¨çš„é“¶ç‰Œå’Œé«˜ä½æ çš„é“œç‰Œã€‚\n2005å¹´æå¨…ç”±äºå—ä¼¤ï¼Œæ²¡æœ‰å‚åŠ ä¸–ç•Œä½“æ“é”¦æ ‡èµ›ï¼Œå¥¹åœ¨æ¾³é—¨ä¸¾è¡Œçš„ç¬¬å››å±Šä¸œäºšè¿åŠ¨ä¼šä¸Šè·å¾—äº†é«˜ä½æ é‡‘ç‰Œã€‚\n2006å¹´3æœˆæå¨…å‚åŠ äº†åœ¨æ¯”åˆ©æ—¶æ ¹ç‰¹ä¸¾è¡Œçš„ä¸–ç•Œæ¯ç³»åˆ—ç«™è·å¾—é«˜ä½æ é‡‘ç‰Œã€‚ä¸¤å‘¨ä»¥ååœ¨å¾·å›½ç§‘ç‰¹å¸ƒæ–¯ä¸¾è¡Œçš„ä¸–ç•Œæ¯ç³»åˆ—ç«™å¥¹ä¹Ÿè·å¾—äº†é«˜ä½æ é‡‘ç‰Œã€‚è¿™ä¸¤æ¬¡äº®ç›¸å¥¹ä»¥ä¸€å¥—å…¨æ–°çš„ï¼Œéš¾åº¦å¾ˆå¤§çš„é«˜ä½æ åŠ¨ä½œæŠ€å‹å…¨åœºï¼ŒæˆåŠŸä½œå‡ºâ€œå‰ç©ºç¿»è½¬ä½“180åº¦æŠ“æ ç›´æ¥æ¥å‰ç©ºç¿»â€ã€‚åŒå¹´ä¸ƒæœˆï¼Œå¥¹åœ¨ä¸­å›½ä¸Šæµ·ä¸¾è¡Œçš„ä¸–ç•Œæ¯ç³»åˆ—ç«™è·å¾—å¹³è¡¡æœ¨é“¶ç‰Œã€‚\næå¨…å‚åŠ äº†2006å¹´10æœˆåœ¨ä¸¹éº¦å¥¥èƒ¡æ–¯ç¬¬39å±Šä½“æ“ä¸–é”¦èµ›ã€‚å¥¹åœ¨é¢„èµ›ä¸­å‘æŒ¥å¤±å¸¸ï¼Œå¹³è¡¡æœ¨ä¸Šåšå¥¹å‡†å¤‡ç”³æŠ¥å‘½åçš„â€œå±ˆä½“å‰ç©ºç¿»è½¬ä½“180åº¦ç«™æœ¨â€æ—¶æ‰ä¸‹å™¨æ¢°ï¼Œä¸ä½†æ²¡èƒ½è·å¾—åŠ¨ä½œçš„å‘½åï¼Œä¹Ÿæ²¡èƒ½è¿›å…¥è¯¥é¡¹ç›®çš„å†³èµ›ã€‚è€Œå¥¹æ“…é•¿è€ŒåŸæœ¬æœ‰å¸Œæœ›å†²å‡»å¥–ç‰Œçš„é«˜ä½æ çš„æ¯”èµ›ä¹Ÿæ˜¯å¤±è¯¯è¿è¿ã€‚ç”±äºå¤±è¯¯ï¼Œå¥¹åŸæœ¬ç”³æŠ¥æ‰“ç®—è·å¾—å‘½åçš„åŠ¨ä½œâ€œå‰ç©ºç¿»è½¬ä½“180æŠ“æ â€ä¹Ÿæœªèƒ½æˆåŠŸä½¿ç”¨ã€‚å³ä½¿è¿™æ ·ï¼Œå¥¹åœ¨é«˜ä½æ é¡¹ç›®ä¸Šç”³æŠ¥çš„å¦å¤–ä¸€ä¸ªåŠ¨ä½œâ€œåæ‘†é˜¿æ‹‰ä¼¯å‰å±ˆä¸¤å‘¨ä¸‹â€è¿˜æ˜¯å¾—åˆ°äº†å›½é™…ä½“æ“è”åˆä¼šçš„è®¤å¯ï¼Œå¹¶ä¸”è·å¾—äº†æ­£å¼å‘½åã€‚æå¨…åœ¨å†³èµ›å‰çš„çƒ­èº«æ´»åŠ¨ä¸­çªç„¶ä¸´é˜µä¸»åŠ¨æå‡ºé€€èµ›ï¼Œæœ€ç»ˆæ•™ç»ƒç»„åœ¨å†³èµ›ä¸­æ²¡æœ‰å®‰æ’æå¨…ä¸Šåœºã€‚æœ€ç»ˆä¸­å›½å¥³é˜Ÿç¬¬ä¸€æ¬¡è·å¾—ä¸–ç•Œä½“æ“é”¦æ ‡å›¢ä½“å† å†›ï¼Œæå¨…ä½œä¸ºå›¢ä½“æˆå‘˜ä¹Ÿè·å¾—ä¸€æšé‡‘ç‰Œã€‚å—æ­¤å½±å“ï¼Œå¥¹ä¹Ÿæ²¡æœ‰å‚åŠ åŒå¹´åœ¨å¡å¡”å°”é¦–éƒ½å¤šå“ˆä¸¾è¡Œçš„2006å¹´äºšæ´²è¿åŠ¨ä¼šã€‚\nä½†æ˜¯ï¼Œæå¨…å¾ˆå¿«ä»å¤±è´¥ä¸­èµ°å‡ºæ¥ã€‚åŒå¹´12æœˆï¼Œåœ¨å·´è¥¿åœ£ä¿ç½—ä¸¾è¡Œçš„ç¬¬13å±Šä¸–ç•Œæ¯æ€»å†³èµ›ä¸Šï¼Œå¥¹åˆæˆåŠŸåšå‡ºäº†å¥¹çš„é«˜éš¾åŠ¨ä½œï¼Œæœ€åè·å¾—é«˜ä½æ é“¶ç‰Œï¼Œè‹±å›½è¿åŠ¨å‘˜è·å¾—äº†è¿™ä¸ªé¡¹ç›®çš„é‡‘ç‰Œã€‚æå¨…è¿˜è·å¾—äº†å¹³è¡¡æœ¨çš„é‡‘ç‰Œã€‚èµ›åï¼Œå›½é™…ä½“æ“è”åˆä¼šå°†â€œå‰ç©ºç¿»è½¬ä½“180åº¦æŠ“æ â€è¿™ä¸ªé«˜éš¾åº¦åŠ¨ä½œå‘½åä¸ºâ€œæå¨…ç©ºç¿»â€ã€‚\n2007å¹´åˆï¼Œæå¨…çš„çŠ¶å†µä»ä¸é”™ï¼Œæ›´äºä¸–ç•Œæ¯é©¬é‡Œåšå°”ç«™å¤ºå¾—é«˜ä½æ é‡‘ç‰Œï¼Œæ‰€ä»¥äº¦è¢«è®¤å®šä¸ºåŒ—äº¬å¥¥è¿ä¼šçš„å¤ºé‡‘å¸Œæœ›ã€‚å¥½æ™¯ä¸å¸¸ï¼Œ2007å¹´7æœˆä¸­æ—¬çš„ä¸€æ¬¡è®­ç»ƒä¸­ï¼Œæå¨…åœ¨åšå‰ç©ºç¿»180åº¦æ¥æŠ“æ å†æ¥å‰ç©ºç¿»æ—¶ï¼Œå› ä¸ºæ²¡èƒ½æŠ“ä½æ ï¼Œæ‘”è½æ—¶æ‰‹æ’‘åœ°ï¼Œå¯¼è‡´è‚˜å…³èŠ‚è„±èŠ‚ã€‚ç»è¿‡æ£€æŸ¥ï¼Œå‘ç°æå¨…æ¡¡éª¨å°å¤´éª¨éªºåŠæ¡¡éª¨ç¯çŠ¶éŸ§å¸¦æŸä¼¤ï¼Œå› æ­¤æå¨…äº¦ç¼ºå¸­ä¹æœˆåˆä¸¾è¡Œçš„ä¸–é”¦èµ›ã€‚\nè‡ªæ­¤ä¹‹åï¼Œå¥¹çš„æ¯”èµ›åŠè®­ç»ƒæ¬¡æ•°éƒ½å‡å°‘ï¼Œç›´è‡³2008å¹´å®£å¸ƒé€€å½¹ï¼Œç»“æŸå¥¹15å¹´çš„ä½“æ“ç”Ÿæ¶¯ã€‚\né€€å½¹ç”Ÿæ¶¯.\n2008å¹´ï¼Œæå¨…é€€å½¹åå…¥è¯»åŒ—äº¬ä½“è‚²å¤§å­¦ï¼Œå¹¶æœ‰æ„å‘å¨±ä¹åœˆå‘å±•ã€‚\n2009å¹´6æœˆ25æ—¥æ™šï¼Œæå¨…åœ¨æŸé¤å…å°±é¤æ—¶ï¼ŒæœåŠ¡å‘˜ä¸Šèœæ—¶ä¸å°å¿ƒæŠŠèœæ±¤æ‰“ç¿»ï¼Œå¯¼è‡´å¥¹åŒè…¿ã€å³è‡‚å¤§é¢ç§¯ä¸¥é‡çƒ«ä¼¤ï¼Œè¢«é‰´å®šä¸ºåçº§ä¼¤æ®‹ã€‚åŠåæ³•é™¢åˆ¤å†³æå¨…è·èµ”6ä¸‡ä½™å…ƒã€‚\n2018å¹´ï¼Œæå¨…æ‹…ä»»ä¸­åé˜Ÿå¥³å­ä½“æ“é˜Ÿæ•™ç»ƒã€‚\n2021å¹´ï¼Œæå¨…æ‹…ä»»å°åŒ—å¸‚æ¾å±±åŒºä¸‰æ°‘å›½æ°‘å°å­¦ä½“æ“é˜Ÿæ•™ç»ƒã€‚\nå‘½ååŠ¨ä½œ.\nä¸€å…±æœ‰ä¸¤ä¸ªä½“æ“åŠ¨ä½œä»¥æå¨…çš„åå­—å‘½åï¼š</document>æå¨…åœ¨å¥¥è¿ä¼šå¤±å¸¸åï¼Œä¸ºä»€ä¹ˆèƒ½å¤Ÿå¿«é€Ÿä»å¤±è´¥ä¸­æ¢å¤å¹¶å–å¾—æ›´å¥½çš„æˆç»©ï¼Ÿ", 
      "æå¨…åœ¨å¥¥è¿ä¼šå¤±å¸¸åèƒ½å¤Ÿå¿«é€Ÿä»å¤±è´¥ä¸­æ¢å¤å¹¶å–å¾—æ›´å¥½çš„æˆç»©ï¼Œå¯èƒ½æœ‰ä»¥ä¸‹å‡ ä¸ªåŸå› ï¼š\n\né¦–å…ˆï¼Œæå¨…å…·å¤‡åšéŸ§çš„å¿ƒç†ç´ è´¨ã€‚ä½œä¸ºä¸€ä½ä½“æ“è¿åŠ¨å‘˜ï¼Œå¥¹ç»å†äº†æ— æ•°æ¬¡çš„æŒ«æŠ˜å’Œå›°éš¾ï¼Œå·²ç»å…»æˆäº†ä¸è½»æ˜“æ”¾å¼ƒçš„å“è´¨ã€‚å¥¹çŸ¥é“å¤±è´¥æ˜¯æˆåŠŸçš„ä¸€éƒ¨åˆ†ï¼Œå¹¶ä¸”èƒ½å¤Ÿä»å¤±è´¥ä¸­å¸å–ç»éªŒæ•™è®­ï¼Œä¿æŒç§¯æçš„å¿ƒæ€å¹¶ç»§ç»­åŠªåŠ›ã€‚\n\nå…¶æ¬¡ï¼Œæå¨…æ‹¥æœ‰è‰¯å¥½çš„è®­ç»ƒå’Œæ•™ç»ƒå›¢é˜Ÿçš„æ”¯æŒã€‚åœ¨å¥¥è¿ä¼šå¤±å¸¸åï¼Œæå¨…å¯èƒ½ç»è¿‡äº†æ·±å…¥çš„åæ€å’Œè®­ç»ƒè°ƒæ•´ã€‚å¥¹çš„æ•™ç»ƒå’Œå›¢é˜Ÿå¯èƒ½å¯¹å¥¹è¿›è¡Œäº†ä¸“é—¨çš„æŒ‡å¯¼å’Œè®­ç»ƒï¼Œå¸®åŠ©å¥¹çº æ­£é—®é¢˜ï¼Œå¹¶æä¾›æ­£ç¡®çš„å¿ƒç†æ”¯æŒå’Œé¼“åŠ±ã€‚\n\næ­¤å¤–ï¼Œæå¨…å¯èƒ½åœ¨æ¯”èµ›ä¸­è¡¨ç°å‡ºæ›´é«˜çš„åŠ¨åŠ›å’ŒåŠ¨ä½œéš¾åº¦ã€‚å¥¥è¿ä¼šå¤±å¸¸åï¼Œå¥¹å¯èƒ½æ„è¯†åˆ°è‡ªå·±åœ¨æŠ€æœ¯å’Œéš¾åº¦æ–¹é¢è¿˜æœ‰æå‡çš„ç©ºé—´ã€‚é€šè¿‡æ›´åŠ åˆ»è‹¦çš„è®­ç»ƒå’Œå°è¯•æ›´å¤šçš„é«˜éš¾åº¦åŠ¨ä½œï¼Œå¥¹èƒ½å¤Ÿåœ¨æ¯”èµ›ä¸­å±•ç°æ›´å¥½çš„è¡¨ç°ï¼Œæé«˜è‡ªå·±çš„æˆç»©ã€‚\n\nç»¼ä¸Šæ‰€è¿°ï¼Œæå¨…ä¹‹æ‰€ä»¥èƒ½å¤Ÿå¿«é€Ÿä»å¥¥è¿ä¼šå¤±å¸¸ä¸­æ¢å¤å¹¶å–å¾—æ›´å¥½çš„æˆç»©ï¼Œå¯èƒ½æ˜¯ç”±äºå¥¹è‰¯å¥½çš„å¿ƒç†ç´ è´¨ã€è®­ç»ƒå›¢é˜Ÿçš„æ”¯æŒä»¥åŠæ›´é«˜çš„åŠ¨åŠ›å’ŒåŠ¨ä½œéš¾åº¦ã€‚è¿™äº›å› ç´ å…±åŒä½œç”¨ï¼Œä½¿å¥¹èƒ½å¤Ÿé€æ­¥å…‹æœæŒ«æŠ˜ï¼Œè·å¾—æ›´å¥½çš„æˆç»©ã€‚",
      "æå¨…å¦‚ä½•åœ¨é€€å½¹åé€‰æ‹©ç»§ç»­æ•™ç»ƒçš„èŒä¸šï¼Œå¹¶ä¸”åœ¨æ•™ç»ƒç”Ÿæ¶¯ä¸­è¡¨ç°å‡ºè‰²ï¼Ÿ", 
      "æå¨…åœ¨é€€å½¹åé€‰æ‹©ç»§ç»­æ•™ç»ƒçš„èŒä¸šå¯èƒ½æ˜¯åŸºäºå¥¹å¯¹ä½“æ“è¿åŠ¨çš„çƒ­çˆ±å’Œå¯¹å¹´è½»è¿åŠ¨å‘˜çš„åŸ¹å…»çš„å…´è¶£ã€‚ä½œä¸ºä¸€ä½é€€å½¹çš„ä½“æ“è¿åŠ¨å‘˜ï¼Œå¥¹æ‹¥æœ‰ä¸°å¯Œçš„ç»éªŒå’Œä¸“ä¸šçŸ¥è¯†ï¼Œå¯ä»¥å°†è¿™äº›å®è´µçš„èµ„æºä¼ æˆç»™æ–°ä¸€ä»£çš„ä½“æ“è¿åŠ¨å‘˜ï¼Œå¹¶å¸®åŠ©ä»–ä»¬å®ç°è‡ªå·±çš„æ¢¦æƒ³ã€‚\n\nåœ¨æ•™ç»ƒç”Ÿæ¶¯ä¸­ï¼Œæå¨…å¯èƒ½è¡¨ç°å‡ºè‰²çš„åŸå› æœ‰ä»¥ä¸‹å‡ ç‚¹ï¼š\n\né¦–å…ˆï¼Œå¥¹æ‹¥æœ‰å‡ºè‰²çš„æŠ€æœ¯å’Œè®­ç»ƒç»éªŒã€‚ä½œä¸ºä¸€ä½é¡¶çº§çš„ä½“æ“è¿åŠ¨å‘˜ï¼Œæå¨…åœ¨è‡ªå·±çš„è¿åŠ¨ç”Ÿæ¶¯ä¸­ç§¯ç´¯äº†ä¸°å¯Œçš„æŠ€æœ¯å’Œè®­ç»ƒçŸ¥è¯†ã€‚å¥¹ç†Ÿæ‚‰å„ç§åŠ¨ä½œçš„è¦é¢†å’Œç»†èŠ‚ï¼Œèƒ½å¤Ÿæ•™æˆç»™å­¦ç”Ÿæ­£ç¡®çš„æŠ€æœ¯å’Œè®­ç»ƒæ–¹æ³•ã€‚\n\nå…¶æ¬¡ï¼Œæå¨…å¯èƒ½å…·å¤‡è‰¯å¥½çš„æ²Ÿé€šèƒ½åŠ›å’Œæ•™å­¦æŠ€å·§ã€‚ä½œä¸ºä¸€åä¼˜ç§€çš„æ•™ç»ƒï¼Œä¸ä»…éœ€è¦æ‹¥æœ‰ä¼˜ç§€çš„æŠ€æœ¯æ°´å¹³ï¼Œè¿˜éœ€è¦èƒ½å¤Ÿä¸å­¦ç”Ÿå»ºç«‹èµ·è‰¯å¥½çš„æ²Ÿé€šå’Œç†è§£ï¼Œäº†è§£ä»–ä»¬çš„éœ€æ±‚å’Œå›°æƒ‘ï¼Œå¹¶èƒ½å¤Ÿç»™äºˆåŠæ—¶å’Œæœ‰æ•ˆçš„æŒ‡å¯¼å’Œå¸®åŠ©ã€‚æå¨…å¯èƒ½é€šè¿‡è‡ªå·±çš„äº²èº«ç»å†ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å­¦ç”Ÿçš„å¿ƒç†å’Œéœ€æ±‚ï¼Œå¹¶ç»™äºˆæ­£ç¡®çš„æŒ‡å¯¼å’Œé¼“åŠ±ã€‚\n\næ­¤å¤–ï¼Œæå¨…å¯èƒ½åœ¨æ•™ç»ƒç”Ÿæ¶¯ä¸­æ³¨é‡åŸ¹å…»å­¦ç”Ÿçš„å…¨é¢ç´ è´¨ã€‚ä½“æ“è¿åŠ¨ä¸ä»…ä»…æ˜¯æŠ€æœ¯çš„è¡¨æ¼”ï¼Œè¿˜éœ€è¦å¯¹èº«ä½“ç´ è´¨ã€çµæ•åº¦ã€åè°ƒæ€§ç­‰å¤šä¸ªæ–¹é¢è¿›è¡Œè®­ç»ƒã€‚æå¨…å¯èƒ½æ³¨é‡åŸ¹å…»å­¦ç”Ÿçš„ç»¼åˆç´ è´¨ï¼Œå¸®åŠ©ä»–ä»¬åœ¨ä½“æ“è¿åŠ¨ä¸­å¾—åˆ°å…¨é¢çš„å‘å±•ã€‚\n\næœ€åï¼Œæå¨…å¯èƒ½å¯¹å­¦ç”Ÿæœ‰ç€è€å¿ƒå’Œå…³çˆ±ã€‚ä½œä¸ºä¸€åä¼˜ç§€çš„æ•™ç»ƒï¼Œéœ€è¦å¯¹å­¦ç”Ÿä¿æŒè€å¿ƒå’Œæ¸©æŸ”ï¼Œå…³æ³¨å­¦ç”Ÿçš„èº«å¿ƒå¥åº·ï¼Œå¹¶ç»™äºˆä»–ä»¬å……åˆ†çš„æ”¯æŒå’Œé¼“åŠ±ã€‚æå¨…å¯èƒ½èƒ½å¤Ÿè¿ç”¨è‡ªå·±çš„äº²èº«ç»å†å’Œç†è§£ï¼Œç»™äºˆå­¦ç”Ÿæ›´å¤šçš„å…³çˆ±å’Œå¸®åŠ©ã€‚\n\nç»¼ä¸Šæ‰€è¿°ï¼Œæå¨…åœ¨é€€å½¹åé€‰æ‹©ç»§ç»­æ•™ç»ƒçš„èŒä¸šï¼Œå¹¶ä¸”åœ¨æ•™ç»ƒç”Ÿæ¶¯ä¸­è¡¨ç°å‡ºè‰²å¯èƒ½æ˜¯ç”±äºå¥¹æ‹¥æœ‰å‡ºè‰²çš„æŠ€æœ¯å’Œè®­ç»ƒç»éªŒã€è‰¯å¥½çš„æ²Ÿé€šèƒ½åŠ›å’Œæ•™å­¦æŠ€å·§ã€æ³¨é‡åŸ¹å…»å­¦ç”Ÿçš„å…¨é¢ç´ è´¨ï¼Œä»¥åŠè€å¿ƒå’Œå…³çˆ±å­¦ç”Ÿã€‚è¿™äº›å› ç´ å…±åŒä½œç”¨ï¼Œä½¿å¥¹èƒ½å¤Ÿåœ¨æ•™ç»ƒå²—ä½ä¸Šå–å¾—å‡ºè‰²çš„è¡¨ç°ã€‚"
      ]
}
```

## Construction of UltraLink
<img src="flow_diagram.png" alt="flow diagram" width="800" style="margin-left:'auto' margin-right:'auto' display:'block'"/>
 In this work, we propose a construction framework consisting of two pipelines. The language-specific pipeline employs a newly introduced knowledge-grounded data augmentation approach to generate conversations with detailed cultural backgrounds. The language-agnostic pipeline leverages a two-stage translation mechanism to effectively utilize the existing English SFT data, with fewer translation errors caused by cultural differences. 
 
### Pipeline 1: Language-Specific Pipeline
The cultures around the world are vibrant and diverse, reflecting the lifestyles and perspectives of people from various countries and regions. To better cater to diverse users, the cultural diversity of multilingual LLMs should be improved. In this work, we propose a knowledge-grounded data augmentation method, leveraging language-specific knowledge bases to provide intricate and varied cultural backgrounds. Our method mainly contains two steps: (1) preparing and sampling knowledge from knowledge bases as cultural backgrounds, and (2) steering LLMs to generate informative conversations given the provided cultural backgrounds.

### Pipeline 2: Language-Agnostic Pipeline
In addition to language-specific abilities, the general abilities that are language-agnostic are also essential for LLMs. As numerous high-quality English SFT datasets already encompass a broad spectrum of general abilities, we suggest employing a two-stage translation mechanism to maximize the utility of existing English resources. Our goal is to reduce translation errors caused by cultural differences, since some questions can not be directly translated into other languages (e.g., write an English poem where each sentence starts with the letter "A"). In the first stage, we introduce a multi-criteria mechanism to filter out English-specific conversations that are difficult to translate accurately into other languages. Then we use GPT-3.5 to translate the remaining language-agnostic data. 
In this study, we consider three key components of general abilities for LLMs: chat, math reasoning, and code generation. For chat, we use ShareGPT as the English chat data, which consists of multi-turn dialogues between human users and ChatGPT. For math reasoning, we use MetaMath as the English math data. For code generation, we use the Magicoder dataset as the English code data.

## To Do
- [x] Upload the data and the model weight
- [ ] Upload the data generation pipeline code
- [ ] Upload the training code

## Citation
Feel free to cite the repo if you think UltraLink is useful.

```bibtex
@misc{wang2024ultralink,
      title={UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset}, 
      author={Haoyu Wang and Shuo Wang and Yukun Yan and Xujia Wang and Zhiyu Yang and Yuzhuang Xu and Zhenghao Liu and Ning Ding and Xu Han and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2402.04588},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```
